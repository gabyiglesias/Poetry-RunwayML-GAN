{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lang_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>stay \\n  xxmaj knowing that you 've a weakness for the girls \\n  xxmaj so xxmaj mr. xxmaj bergman , xxmaj mr. b , how 'bout a cup of herbal tea \\n  xxmaj you 've had a stressful day , time to unwind , so unwind \\n \\n  xxmaj no , let us talk not of films but of your life in xxmaj xxunk xxmaj hills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>— the only brand my grandma smokes — and the faintest hint of xxmaj coppertone . \\n \\n  i 'm watching xxunk circle in real close . i know you 're gon na go . \\n \\n  xxmaj just please leave me a note . i left because you asked me to . \\n \\n  xxmaj operator , take me home . i do n't know where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>shalalalala ) \\n  xxmaj these were our greatest hits ( xxmaj shalalalalalalala xxmaj shalalalala ) \\n  xxmaj the best of me and you \\n \\n  i still remember buying you ' xxmaj band on the xxmaj run ' on the first day that we \\n  xxmaj kissed \\n  xxmaj but you always did prefer ' xxmaj mccartney i ' because it reminded you of being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>happen to you \\n  xxmaj wake up and stumble , trip and fall \\n  xxmaj you had so much \\n  xxmaj and now you 've lost it all \\n \\n  xxmaj busted down and broken all in two \\n  xxmaj but you never thought \\n  this would happen to you \\n  xxmaj wake up and stumble , trip and fall \\n  xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>stole the show \\n  xxmaj long ago , long ago \\n  xxmaj nights turned slow \\n  xxmaj thunder would roll \\n \\n  xxmaj winters rise white as snow \\n  xxmaj right as i turn around to go \\n  xxmaj the wind blows , the wind blows \\n  xxmaj caught in the cold \\n  a wheel for my shoulder \\n \\n  xxmaj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = load_data(path, 'tmp_lyrics')\n",
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = data_lm.train_dl\n",
    "val_dl = data_lm.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lm_loss(input, target, kld_weight=0):\n",
    "    sl, bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_drop_lin(n_in, n_out, bn=True, initrange=0.01,p=0, bias=True, actn=nn.LeakyReLU(inplace=True)):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    linear = nn.Linear(n_in, n_out, bias=bias)\n",
    "    if initrange:linear.weight.data.uniform_(-initrange, initrange)\n",
    "    if bias: linear.bias.data.zero_()\n",
    "    layers.append(linear)\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/projects/fastai/lib/python3.6/site-packages/fastai/text/learner.py:209: UserWarning: There are no pretrained weights for that architecture yet!\n",
      "  warn(\"There are no pretrained weights for that architecture yet!\")\n"
     ]
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm, arch=TransformerXL)\n",
    "#learn.load('lyrics_fine_tuned_novel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerXL(\n",
       "  (encoder): Embedding(60004, 410)\n",
       "  (pos_enc): PositionalEncoding()\n",
       "  (drop_emb): Dropout(p=0.1)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): DecoderLayer(\n",
       "      (mhra): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "        (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "        (drop_att): Dropout(p=0.1)\n",
       "        (drop_res): Dropout(p=0.1)\n",
       "        (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "      )\n",
       "      (ff): SequentialEx(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "          (1): ReLU(inplace)\n",
       "          (2): Dropout(p=0.1)\n",
       "          (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "          (4): Dropout(p=0.1)\n",
       "          (5): MergeLayer()\n",
       "          (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = deepcopy(learn.model[0])\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(trn_dl))\n",
    "x.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70, 410])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[-1][-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([64, 70, 410])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[out.size() for out in outs[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = deepcopy(learn.model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(learn.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TextDicriminator(nn.Module):\n",
    "    def __init__(self,encoder, nh, bn_final=True):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        self.encoder = encoder\n",
    "        #classifier\n",
    "        layers = []\n",
    "        layers+=bn_drop_lin(nh*3,nh,bias=False)\n",
    "        layers += bn_drop_lin(nh,nh,p=0.25)\n",
    "        layers+=bn_drop_lin(nh,1,p=0.15,actn=nn.Sigmoid())\n",
    "        if bn_final: layers += [nn.BatchNorm1d(1)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def pool(self, x, bs, is_max):\n",
    "        f = F.adaptive_max_pool1d if is_max else F.adaptive_avg_pool1d\n",
    "        return f(x.permute(0,2,1), (1,)).view(bs,-1)\n",
    "    \n",
    "    def forward(self, inp,y=None):\n",
    "        raw_outputs, outputs = self.encoder(inp)\n",
    "        output = outputs[-1]\n",
    "        bs,sl,_ = output.size()\n",
    "        avgpool = self.pool(output, bs, False)\n",
    "        mxpool = self.pool(output, bs, True)\n",
    "        x = torch.cat([output[:,-1], mxpool, avgpool], 1)\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = TextDicriminator(encoder,400).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(disc.parameters(), lr = 3e-4)\n",
    "optimizerG = optim.Adam(generator.parameters(), lr = 3e-3, betas=(0.7, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def seq_gumbel_softmax(input):\n",
    "    samples = []\n",
    "    bs,sl,nc = input.size()\n",
    "    for i in range(sl): \n",
    "        z = F.gumbel_softmax(input[:,i,:])\n",
    "        samples.append(torch.multinomial(z,1))\n",
    "    samples = torch.stack(samples).transpose(1,0).squeeze(2) \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reinforce_loss(input,sample,reward):\n",
    "    loss=0\n",
    "    bs,sl = sample.size()\n",
    "    for i in range(sl):\n",
    "        loss += -input[:,i,sample[:,i]] * reward\n",
    "    return loss/sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def step_gen(ds,gen,disc,optG,crit=None):\n",
    "    gen.train(); disc.train()\n",
    "    x,y = ds\n",
    "    bs,sl = x.size()\n",
    "    fake,_,_ = gen(x)\n",
    "    gen.zero_grad()\n",
    "    fake_sample =seq_gumbel_softmax(fake)\n",
    "    with torch.no_grad():\n",
    "        gen_loss = reward = disc(fake_sample)\n",
    "        if crit: gen_loss = crit(fake,fake_sample,reward.squeeze(1))\n",
    "        gen_loss = gen_loss.mean()\n",
    "    gen_loss.requires_grad_(True)\n",
    "    gen_loss.backward()\n",
    "    optG.step()\n",
    "    return gen_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def step_disc(ds,gen,disc,optD,d_iters):\n",
    "    for j in range(d_iters):\n",
    "        gen.eval(); disc.train()\n",
    "        with torch.no_grad():\n",
    "            fake,_,_ = gen(x)\n",
    "            fake_sample = seq_gumbel_softmax(fake)\n",
    "        disc.zero_grad()\n",
    "        fake_loss = disc(fake_sample)\n",
    "        real_loss = disc(y.view(bs,sl))\n",
    "        disc_loss = (fake_loss-real_loss).mean(0)\n",
    "        disc_loss.backward()\n",
    "        optimizerD.step()\n",
    "    return disc_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def evaluate(ds,gen,disc,crit=None):\n",
    "    with torch.no_grad():\n",
    "        x, y = ds\n",
    "        bs,sl = x.size()\n",
    "        fake,_,_ = gen(x)\n",
    "        fake_sample =seq_gumbel_softmax(fake)\n",
    "        gen_loss = reward = disc(fake_sample)\n",
    "        if crit: gen_loss = crit(fake,fake_sample,reward.squeeze(1))\n",
    "        gen_loss = gen_loss.mean()\n",
    "        fake_sample = seq_gumbel_softmax(fake)\n",
    "        fake_loss = disc(fake_sample).mean(0).view(1).data.item()\n",
    "        real_loss = disc(y.view(bs,sl)).mean(0).view(1).data.item()\n",
    "        disc_loss = (fake_loss-real_loss).mean(0).view(1).data.item()\n",
    "    return fake,gen_loss,disc_loss,fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(gen, disc, epochs, trn_dl, val_dl, optimizerD, optimizerG, crit=None,first=True):\n",
    "    gen_iterations = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        gen.train(); disc.train()\n",
    "        n = len(trn_dl)\n",
    "        #train loop\n",
    "        with tqdm(total=n) as pbar:\n",
    "            for i, ds in enumerate(trn_dl):\n",
    "                gen_loss = step_gen(ds,gen,disc,optimizerG,crit)\n",
    "                gen_iterations += 1\n",
    "                d_iters = 3\n",
    "                disc_loss = step_disc(ds,gen,disc,optimizerD,d_iters)\n",
    "                pbar.update()\n",
    "        print(f'Epoch {epoch}:')\n",
    "        print('Train Loss:')\n",
    "        print(f'Loss_D {disc_loss}; Loss_G {gen_loss} Ppx {torch.exp(lm_loss(fake,y))}')\n",
    "        print(f'D_real {real_loss}; Loss_D_fake {fake_loss}')\n",
    "        \n",
    "        disc.eval(), gen.eval()\n",
    "        with tqdm(total=len(val_dl)) as pbar:\n",
    "            for i, ds in enumerate(val_dl):\n",
    "                fake,gen_loss,disc_loss,fake_loss = evaluate(ds,gen,disc,crit)\n",
    "                pbar.update()\n",
    "        print('Valid Loss:')\n",
    "        print(f'Loss_D {disc_loss}; Loss_G {gen_loss} Ppx {torch.exp(lm_loss(fake,ds[-1]))}')\n",
    "        print(f'D_real {real_loss}; Loss_D_fake {fake_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "nh = {'AWD':400,'XL':410}\n",
    "crits={'gumbel':None,'reinforce':reinforce_loss}\n",
    "\n",
    "#train a language model with gan objective\n",
    "def run(path,filename,pretrained,model,crit=None,preds=True,epochs=6):\n",
    "    \n",
    "    #load data after running preprocess\n",
    "    print(f'loading data from {path}/{filename};')\n",
    "    data_lm = load_data(path, filename)\n",
    "    trn_dl = data_lm.train_dl\n",
    "    val_dl = data_lm.valid_dl\n",
    "    \n",
    "    #select encoder for model\n",
    "    print(f'training text gan model {model}; pretrained from {pretrained};')\n",
    "    learn = language_model_learner(data_lm, arch=models[model])\n",
    "    learn.load(pretrained)\n",
    "    encoder = deepcopy(learn.model[0])\n",
    "    \n",
    "    generator = deepcopy(learn.model)\n",
    "    generator.load_state_dict(learn.model.state_dict())\n",
    "    disc = TextDicriminator(encoder,nh[model]).cuda()\n",
    "    \n",
    "    disc.train()\n",
    "    generator.train()\n",
    "    \n",
    "    #create optimizers\n",
    "    optimizerD = optim.Adam(disc.parameters(), lr = 3e-4)\n",
    "    optimizerG = optim.Adam(generator.parameters(), lr = 3e-3, betas=(0.7, 0.8))\n",
    "    \n",
    "    print(f'training for {epochs} epochs')\n",
    "    train(generator, disc, epochs, trn_dl, val_dl, optimizerD, optimizerG, first=False)\n",
    "    \n",
    "    #save model\n",
    "    learn.model.load_state_dict(generator.state_dict())\n",
    "    print(f'saving model to {path}/{filename}_{model}_gan_{crit}')\n",
    "    learn.save(filename+'_'+model+'_gan_'+crit)\n",
    "    \n",
    "    #generate output from validation set\n",
    "    if preds:\n",
    "        print(f'generating predictions and saving to {path}/{filename}_{model}_preds.txt;')\n",
    "        get_valid_preds(learn,data_lm,filename+'_'+model+'_preds.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "if __name__ == '__main__': fire.Fire(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted textgan.ipynb to ../textgan.py\r\n"
     ]
    }
   ],
   "source": [
    "!/home/ubuntu/projects/creativity-model-zoo/notebooks/notebook2script.py textgan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
